---
layout: post
title: "[01] Reinforcement Learning, 강화학습"
categories: [AI, ReinforcementLearning]
tags: [AI, ReinforcementLearning]
fullview: false
comments: false
use_math: true
---

본 포스팅은 **"Do it! 강화 학습 입문"** - (이지스퍼블리싱, 조규남, 맹윤호, 임지순 지음)을 바탕으로 공부한 내용을 정리한 것입니다.

1. 강화학습
2. 마르코프 결정 (MDP, Markov Decision Process)
    + 영화를 예시로 바라보는 마르코프 결정
    + 상태, 행동, 보상
    + 행동에 의한 상태 전이에 확률 도입

---

# 1. 강화학습 (Reinforcement Learning)

머신러닝 (Machine Learning)은 크게 다음과 같이 나눌 수 있다.

1. 지도학습 (Supervised Learning)
    + 데이터와 레이블이 주어진 상태에서, 새 데이터에 레이블을 매기는 방법을 학습.
2. 비지도학습 (Un-Supervised Learning)
    + 레이블 없이 데이터만 주어진 상태에서, 새 데이터의 분류, 밀도 추정 등의 방법을 학습. 특징을 요약할 때 등.
3. 강화학습 (Reinforcement Learning)
    + 행동과 보상으로, 어떤 상태에서 최적의 행동을 찾아가는 방법을 학습.

강화학습은 지도, 비지도 학습과 다르게 논리와 수학 분야를 바탕으로 

> " 상호 작용을 하면서 학습할 때 필요한 계산적 접근 방법"

---

# 2. 마르코프 결정 (MDP, Markov Decision Process)

> **마르코프 결정 과정 (MDP, Markov Decision Process)**은 의사 결정을 하는데 필요한 체계로 볼 수 있다.

## 영화를 예시로 바라보는 마르코프 결정

책에서는 이를 설명하기 위해 예시로 영화 **[엣지 오브 투모로우](https://movie.naver.com/movie/bi/mi/basic.naver?code=90589 "네이버 영화 검색")**를 들고 있다.

> # 엣지 오브 투모로우 (Edge of Tomorrow, 2014)
> 
> ![image](https://user-images.githubusercontent.com/84369912/130061856-d1bda510-dbed-42e5-bd5d-797845888a7e.png)
> 주인공 빌 케이지(톰 크루즈)는 외계인이 점령한 지구를 해방하기 위한 인류 연합군에서 상륙작전에 투입되고, 우연히 외계인의 **시간 리셋** 능력을 얻고
> 죽을 때마다 작전의 전날에 부활한다. 케이지는 전투와 사망을 수백 번 반복하며, 인류의 승리를 위해 싸워나간다.

강화학습의 관점에서 해당 줄거리를 바라보았을 때, 다음 그림과 같이 나타낼 수 있다.

![image](https://user-images.githubusercontent.com/84369912/130061782-5f5b3149-4bb3-40f8-9afc-0813863ade2f.png)

강화학습에서 **에이전트 (Agent)**는 해당 예시에서 행동의 주체인 주인공으로 볼 수 있다.

**에이전트**는 전투라는 **환경**에서 **상호 작용**하며 가장 큰 **보상**인 외계인 대장 제거를 위해, 

죽을 때마다 작전 전날에 부활하며 **초기 상태**에서 출발하여, 

새로운 **에피소드**에서 다양한 **상태**와 그 때의 **행동**을 통해 대장을 물리치는 과정을 **탐색**한다.

![image](https://user-images.githubusercontent.com/84369912/130063613-786fcf98-ebab-41f7-a2c7-f4e52a5c374a.png)

$$s_{1}$$이나 $$a_{1-1}$$과 같은 표기는 각각 **상태 (state)**와 **행동 (action**)을 의미한다.

- 본진($$s_{1}$$)에서 깨어난 에이전트는 상륙 작전 전투에 참가($$a_{1}$$), 본진 이탈($$a_{2}$$)가 가능.
- 상륙 작전 투입($$s_{2}$$)된 에이전트는 혼자 전투($$a_{2-1}$$), 전투를 도울 영웅을 구출($$a_{2-2}$$)가 가능.
- 본진 이탈($$s_{5}$$)한 에이전트는 다시 상륙 작전($$a_{5-1}$$)을 향할 수도, 비밀을 조사하러 후방 연구소로 이동($$a_{5-2}$$) 가능.
- 후방 연구소 이동($$s_{6}$$)한 에이전트는 전투 훈련($$a_{6-1}$$)을 받을 수도, 외계인 보스 조사($$a_{6-2}$$)도 가능.

## 상태, 행동, 보상

이런 식으로 에이전트는 시작 상태와 종단 상태를 포함한 7가지 상태, 그 둘을 제외한 각 상태에서 2개의 행동을 취할 수 있다.

각 상태 $$s_{1}, s_{2}, ..., s_{7}$$의 상태 집합을 $$S$$로 표현하고, 행동의 집합을 $$A(s)$$로 표현.

$$s_{1}$$에서 취할 수 있는 행동의 집합은 $$A(s_{1})$$이 된다.

그림에서 각 상태에서의 보상이 명시 되어있다. 

$$R(s_{3})=-1, R(s_{4})=-1, R(s_{7})=1$$ 

$$R(s_{1})=R(s_{2})=R(s_{5})=R(s_{6})=0$$

## 행동에 의한 상태 전이에 확률 도입

에이전트가 $$s_{1}$$에서 $$a_{1-2}$$를 했을 때, 상태 전이에는 확률이 있다.
80% 확률로 $$s_{5}$$, 
20% 확률로 $$s_{1}$$로 전이될 수도 있다.

상태 전이 확률 (State Transition Probablity)는 수식으로 표현가능 하며, 상태-행동 한 쌍의 전이 확률 합은 1이다.

$$P(s_{5} \mid s_{1},a_{1-2})=0.8$$

$$P(s_{1} \mid s_{1},a_{1-2})=0.8$$

이런 확률의 집합을 모델 (Model)이라 한다. 표로 이들을 표현하면 다음과 같다.

| :---: | :--- |
| 상태 (Status, S) | 에이전트가 환경 내 특정 시점에서 관찰할 수 있는 것을 수치화 |
| 행동 (Action, A) | 에이전트가 환경에게 전달하는 입력 |
| 보상 (Reward, R) | 에이전트가 환경으로부터 전달받은 목적을 달성하기 위해 행동 수행에 대한 피드백 |
| 모델 (Model, M) | 행동에 따른 상태 전이가 일어날 확률을 담은 규칙 |

즉, **MDP**는 상태 집합 $$S$$로 이루어지며, 행동 집합 $$A$$가 있고, 

상태 $$s$$에서 행동 $$a$$를 통해 상태 $$s^\prime$$로 전이할 확률은 $$P(s^\prime \mid s,a)$$이며, 보상은 $$R(s,a)$$이다.

> 마르코프 특성 (Markov Property)를 만족한다고 가정하면 모든 상태는 직전의 상태와 행동에 의해서만 결정된다.
> MDP에서는 에이전트의 오직 현재 상태만 다음 상태 전이 확률에 영향을 미친다. '과거는 상관없다'

---

# 참고

**"Do it! 강화 학습 입문"** - (이지스퍼블리싱, 조규남, 맹윤호, 임지순 지음)

- 깃허브: <https://github.com/yunho0130/start-RL>