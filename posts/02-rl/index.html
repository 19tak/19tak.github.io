<!DOCTYPE html><html lang="ko-KR" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="[RL] 모델 프리 강화 학습" /><meta property="og:locale" content="ko_KR" /><meta name="description" content="본 포스팅은 “Do it! 강화 학습 입문” - (이지스퍼블리싱, 조규남, 맹윤호, 임지순 지음)을 바탕으로 공부한 내용을 정리한 것입니다." /><meta property="og:description" content="본 포스팅은 “Do it! 강화 학습 입문” - (이지스퍼블리싱, 조규남, 맹윤호, 임지순 지음)을 바탕으로 공부한 내용을 정리한 것입니다." /><link rel="canonical" href="https://19tak.github.io/posts/02-rl/" /><meta property="og:url" content="https://19tak.github.io/posts/02-rl/" /><meta property="og:site_name" content="19tak" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-08-20T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[RL] 모델 프리 강화 학습" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-10-16T16:26:29+09:00","datePublished":"2021-08-20T00:00:00+09:00","description":"본 포스팅은 “Do it! 강화 학습 입문” - (이지스퍼블리싱, 조규남, 맹윤호, 임지순 지음)을 바탕으로 공부한 내용을 정리한 것입니다.","headline":"[RL] 모델 프리 강화 학습","mainEntityOfPage":{"@type":"WebPage","@id":"https://19tak.github.io/posts/02-rl/"},"url":"https://19tak.github.io/posts/02-rl/"}</script><title>[RL] 모델 프리 강화 학습 | 19tak</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="19tak"><meta name="application-name" content="19tak"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="https://user-images.githubusercontent.com/84369912/127220954-701d3e97-6d9d-447a-b99f-1a31e4b7b03d.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title"> <a href="/">19tak</a></div><div class="site-subtitle">Peace n Love</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>홈</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>카테고리</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>태그</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>아카이브</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>정보</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/19tak" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://www.instagram.com/taaaktaaaak" aria-label="instagram" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a> <a href="https://www.youtube.com/channel/UCse2sGik0zU4fVNZyDjhcOQ" aria-label="youtube" target="_blank" rel="noopener"> <i class="fab fa-youtube"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['wjdxkrdl123','naver.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> 홈 </a> </span> <span>[RL] 모델 프리 강화 학습</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> 포스트</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="검색..."> </span> <span id="search-cancel" >취소</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>[RL] 모델 프리 강화 학습</h1><div class="post-meta text-muted"> <span> 게시 <em class="" data-ts="1629385200" data-df="YYYY-MM-DD" data-toggle="tooltip" data-placement="bottom"> 2021-08-20 </em> </span> <span> 업데이트 <em class="" data-ts="1665905189" data-df="YYYY-MM-DD" data-toggle="tooltip" data-placement="bottom"> 2022-10-16 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/19tak">19tak</a> </em> </span><div></div></div></div><div class="post-content"><p>본 포스팅은 <strong>“Do it! 강화 학습 입문”</strong> - (이지스퍼블리싱, 조규남, 맹윤호, 임지순 지음)을 바탕으로 공부한 내용을 정리한 것입니다.</p><ol><li>모델 기반 vs 모델 프리<li>몬테카를로 학습 (MC, Monte Carlo Learning)<ul><li>몬테카를로 학습의 개념<li>몬테카를로 학습의 특징</ul><li>시간차 학습 (TD, Temporal Difference Learning)<li>Q 학습 (Q-Learning)</ol><hr /><h1 id="1-모델-기반-vs-모델-프리">1. 모델 기반 vs 모델 프리</h1><p><strong>모델 기반 (Model-Based) 강화 학습</strong>은 에이전트가 환경 안에서 가질 수 있는</p><p>모든 상태와 각 상태 간의 전이 확률을 미리 안다는 전제가 깔려 있다.</p><p>MDP를 이루는 구성 요소 중 모델에 대한 모든 지식을 미리 알고 있다는 것이다.</p><p>종단 상태로부터 역추적해 모든 상태와 행동의 가치를 계산하는 완전 탐색이라는 특징까지도,</p><p>가치 반복법과 정책 반복법은 현실 세계에 적용하기 어렵다.</p><h3 id="그렇다면"><span class="mr-2">그렇다면</span><a href="#그렇다면" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>종단 상태로부터 역추적이 아닌, 초기 상태로부터 탐색을 진행하며 경험을 누적하는 정책은?</p><p>완전 탐색이 필요하지 않으므로 계산양을 크게 줄일 수 있다.</p><p>이렇게 모델에 대한 완전한 지식이 필요치 않은 <strong>모델 프리 (Model-Free) 강화 학습</strong>에는</p><p>대표적으로 <strong>몬테카를로 (MC, Monte Carlo) 학습</strong>과 <strong>시간차 (Temporal Difference) 학습</strong>이 있다.</p><hr /><h1 id="2-몬테카를로-학습-mc-monte-carlo-learning">2. 몬테카를로 학습 (MC, Monte Carlo Learning)</h1><h2 id="몬테카를로-학습의-개념"><span class="mr-2">몬테카를로 학습의 개념</span><a href="#몬테카를로-학습의-개념" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>완전한 가치 함수를 구할 수 없으므로, 가치 함수의 추정치를 구하는 방식으로 문제 해결에 접근한다.</p><p>정책을 초기에 무작위로 설정하고, 이를 이용해 가치 함수를 추정하고, 이로 정책을 개선한다.</p><p>개선한 정책으로 가치 함수를 추정하고, 추정한 가치 함수로 다시 정책을 개선한다.</p><p>가치 함수와 정책이 모두 수렴할 때까지 반복하는데 <strong>정책 반복법</strong>과는 큰 차이점이 있다.</p><ol><li>MC는 완전한 가치 함수가 아닌 가치 함수의 추정치만 얻을 수 있다.<li>MC는 환경내 모든 상태의 가치 추정이 아닌, 에이전트가 거쳐간 상태의 가치 함수만 추정한다.</ol><p>이렇게 에이전트가 직접 에피소드를 겪어 나가며 모델을 추정하기 때문에,</p><p>환경의 모든 상태를 거친다고 보장하지 못한다.</p><h2 id="몬테카를로-학습의-특징"><span class="mr-2">몬테카를로 학습의 특징</span><a href="#몬테카를로-학습의-특징" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ol><li>알려진 모델이 없다고 가정. 즉, 에이전트는 주어진 상태에서 행동을 취했을 때, 어떤 상태로 전이할지, 보상이 주어질지 모른다.<li>에이전트는 경험의 표본으로부터 학습.<li>현재까지 겪은 모든 <strong>에피소드</strong>에 대해 상태의 이익 \(G\)를 평균하여 상태의 가치 함수를 구함. (경험적 평균)<li><strong>에피소드</strong> 하나를 완전히 끝낸 다음 업데이트.<li><strong>에피소드</strong> 단위 문제에 한하여 적용 가능.</ol><p>이렇듯 MC의 핵심은 에피소드이고, 기본 학습 단위가 에피소드 완주이므로 무한한 환경에서는 적용이 불가하다.</p><p>또한, 모델을 모르기 때문에, 전이 확률을 모르고, 따라서 가치 함수의 의미가 없다.</p><p>즉, 특정한 행동이 그 상태의 최선 행동이라 말할 수 없다. 그래서 나온 것이,</p><p>상태 \(s\)에서 행동 \(a\)를 취했을 때의 장기적 보상에 대한 평균, 행동-가치 함수, <strong>Q 함수</strong> \(Q(s,a)\)다.</p><blockquote><p>상태 \(s\)만을 입력으로 하는 가치 함수 \(V(s)\)에 행동 \(a\)를 추가했다고 생각하자.</p></blockquote><hr /><h1 id="3-시간차-학습-td-temporal-difference-learning">3. 시간차 학습 (TD, Temporal Difference Learning)</h1><p>에피소드가 무한히 지속되거나 하나의 에피소드가 종료되기까지 시간이 오래 걸릴때,</p><p>에피소드 자체가 길다면 <strong>시간차 (TD) 학습</strong>을 사용하자.</p><p>MC는 하나의 에피소드가 끝날 때 마다 Q 함수를 업데이트 한다면,</p><p>TD는 상태 변화가 있을 때마다 업데이트한다.</p><p>하나의 에피소드가 끝나지 않아도, 단계마다 학습이 가능하다는 뜻이다.</p><h2 id="mc-td와-분산-편향"><span class="mr-2">MC, TD와 분산, 편향</span><a href="#mc-td와-분산-편향" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><strong>분산 (Variance)</strong>은 학습 데이터가 얼마나 넓게 퍼져있는지를 나타내고,</p><p>분산이 크면 잡음이나 오류가 많이 포함되고 <strong>과대 적합 (overfitting)</strong>이 발생한다.</p><p><strong>편향 (Bias)</strong>은 데이터의 일부만을 학습해 잘못된 가정이 만들어지는 것이고,</p><p><strong>과소 적합 (underfitting)</strong> 문제가 발생할 수 있다.</p><p>MC는 에피소드가 끝나야만 Q 함수를 업데이트 가능하고,</p><p>에피소드마다 서로 다른 상태를 갈 수 있으므로 분산이 크고 편향이 작다.</p><p>TD는 다음 단계의 영향만 받아 가치 함수를 업데이트 하므로, 분산이 작고 편향이 크다.</p><p>이러한 문제를 인지하고 넘어가자.</p><h2 id="수식"><span class="mr-2">수식</span><a href="#수식" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>MC에서 \(N\)회의 에피소드에 대한 \(G_{t+1}\)값에 할인율 \(\gamma\)를 곱한 다음 평균하여 \(Q(S,A)\)를 구했다.</p>\[Q(S,A)=(1-1/N)Q(S,A)+\gamma G_{t+1}/N\]<p>여기서 \(1-1/N\)을 \(\alpha\)로 치환하면 다음과 같다.</p>\[Q(S,A)=Q(S,A)+\alpha (\gamma G_{t+1}-Q(S,A))\]<p>TD의 Q 함수 업데이트 공식도 이와 유사하다.</p><p>TD는 에피소드 종료 이전에 업데이트가 가능하므로 \(G_{t+1}\)을 구할 수 없고, \(\alpha\)도 \(1-1/N\)일 필요가 없다.</p><p>다음 단계 보상 \(R_{t+1}\)과 \(Q(S_{t+1},A_{t+1})\)가 반환하는 값으로 현재의 \(Q(S_{t},A_{t})\)를 업데이트 한다.</p>\[Q(S_{t},A_{t})=Q(S_{t},A_{t})+\alpha (R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S,A))\]<h2 id="엡실론-탐욕-epsilon-greedy-알고리즘"><span class="mr-2">엡실론-탐욕 (Epsilon-Greedy) 알고리즘</span><a href="#엡실론-탐욕-epsilon-greedy-알고리즘" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>모델 프리 학습들은 에이전트가 거치는 상태와 행동에만 Q 함수를 갱신하므로, 샘플링으로 볼 수 있다.</p><p>\(A_{t+1}\)을 선택하는 기준은 무엇이 있을까? 그냥 Q를 최대로 할까?</p><p>가치 반복법과 같이 더 큰 Q 함수, 가치 함수를 얻기 위한 행동을 선택하는 것을</p><p><strong>탐욕 알고리즘 (Greedy Algorithm)</strong>이라 한다.</p><p>상태, 행동 공간이 넓은 환경에서는 탐욕 알고리즘이 좋을 수 있다.</p><p>하지만 MC, TD 모두 자신의 상태, 행동을 샘플링하여 학습에 반영하기 때문에</p><p>샘플링 범위를 줄이는 탐욕 알고리즘이 최적의 정책을 찾을 가능성을 낮춘다.</p><p>이 때, 에이전트가 가치 함수를 이용하며, 새로운 영역을 탐험할 수 있도록 여지를 주면</p><p>최적의 행동과 정책을 찾아낼 가능성이 높아질 것으로 볼 수 있다.</p><p><img data-src="https://user-images.githubusercontent.com/84369912/130141909-0b12ba6a-ea7c-402f-bd98-c9a5daf64f14.png" alt="image" data-proofer-ignore></p><p>이를 <strong>엡실론-탐욕 (Epsilon-Greedy) 알고리즘</strong>이라 한다.</p><div class="table-wrapper"><table><tbody><tr><td>탐욕 <br /> 알고리즘<td>가장 높은 가치 함수 또는 Q 함수를 추구하는 방향으로 행동을 결정. <br /> 탐험을 배제한 이용.<tr><td>랜덤 노이즈 <br /> 알고리즘<td>에이전트가 지나는 각 단계마다 가치 함수의 추정치에 무작위 값을 더함. <br /> 더한 노이즈로 탐험을 진행<tr><td>엡실론-탐욕 <br /> 알고리즘<td>엡실론 확률을 통해 <br /> 가장 높은 가치 함수를 추구하지 않는 행동중에 하나를 무작위로 선택.</table></div><p>TD에서는 엡실론-탐욕 알고리즘을 통해 탐색이 덜 진행된 상태에서 학습 결과가 수렴하는 것을 막을 수 있다.</p><hr /><h1 id="4-q-학습-q-learning">4. Q 학습 (Q-Learning)</h1><h2 id="행동-정책과-학습-정책"><span class="mr-2">행동 정책과 학습 정책</span><a href="#행동-정책과-학습-정책" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>TD에서 임의의 정책인 \(\pi (s)\)를 사용해서 행동 \(A_{t}\)를 취하고,</p><p>다음 단계에서 \(Q\)를 추정할 때 같은 \(\pi (s)\)를 사용하여 행동 \(A_{t+1}\)을 선택한다.</p><p>움직이기 위한 정책 (행동정책)과 학습하기 위한 정책 (학습 정책)이 같다는 뜻이다.</p><p>행동 정책에 탐험 요소를 어느 정도 넣는 것은 바람직하지만,</p><p>학습 수렴 속도가 느려지기때문에 학습 정책에는 탐험 요소가 필요치 않다.</p><p>둘 모두를 탐욕 알고리즘을 사용하면 학습은 빠르게 수렴하지만,</p><p>탐험 요소가 없으므로 최적 정책 수렴은 힘들다. Q 학습은 여기서 고안 되었다.</p><h2 id="q-학습은"><span class="mr-2">Q 학습은</span><a href="#q-학습은" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>행동 정책과 학습 정책을 서로 다르게 하는 Q 학습의 Q 함수 업데이트 공식은</p><p>앞선 TD 학습의 Q 함수 업데이트 수식과 거의 비슷하다.</p>\[Q(S_{t},A_{t})=(1-1/N)Q(S_{t},A_{t})+(R_{t+1}+\gamma max_{a}Q(S_{t+1},a))/N\]<p>수식에서 우리는 행동 정책 (행동 \(A_{t}\)를 선택)과 학습 정책(\(a\)를 선택)이 다름을 볼 수 있다.</p><p>행동 정책은 엡실론-탐욕, 학습 정책은 탐욕 알고리즘을 따른다.</p><p>DQN (Deep Q-Networks)가 바로 이 Q 학습에 딥러닝을 접목한 것이다.</p><hr /><h1 id="참고">참고</h1><p><strong>“Do it! 강화 학습 입문”</strong> - (이지스퍼블리싱, 조규남, 맹윤호, 임지순 지음)</p><ul><li>깃허브: <a href="https://github.com/yunho0130/start-RL">https://github.com/yunho0130/start-RL</a></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/study/'>Study</a>, <a href='/categories/ai/'>AI</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a> <a href="/tags/reinforcementlearning/" class="post-tag no-text-decoration" >ReinforcementLearning</a> <a href="/tags/monte-carlo/" class="post-tag no-text-decoration" >Monte Carlo</a> <a href="/tags/temporal-difference/" class="post-tag no-text-decoration" >Temporal Difference</a> <a href="/tags/model-free-reinforcement-learning/" class="post-tag no-text-decoration" >Model Free Reinforcement Learning</a> <a href="/tags/model-free/" class="post-tag no-text-decoration" >Model Free</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> 이 기사는 저작권자의 <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> 라이센스를 따릅니다.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">공유하기</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%5BRL%5D+%EB%AA%A8%EB%8D%B8+%ED%94%84%EB%A6%AC+%EA%B0%95%ED%99%94+%ED%95%99%EC%8A%B5+-+19tak&url=https%3A%2F%2F19tak.github.io%2Fposts%2F02-rl%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%5BRL%5D+%EB%AA%A8%EB%8D%B8+%ED%94%84%EB%A6%AC+%EA%B0%95%ED%99%94+%ED%95%99%EC%8A%B5+-+19tak&u=https%3A%2F%2F19tak.github.io%2Fposts%2F02-rl%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2F19tak.github.io%2Fposts%2F02-rl%2F&text=%5BRL%5D+%EB%AA%A8%EB%8D%B8+%ED%94%84%EB%A6%AC+%EA%B0%95%ED%99%94+%ED%95%99%EC%8A%B5+-+19tak" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="링크 복사하기" data-title-succeed="링크가 복사되었습니다!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">최근 업데이트</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/03-process_modeling/">[IT] 프로세스 모델링 이해 03 - 분석 개요</a><li><a href="/posts/04-process_modeling/">[IT] 프로세스 모델링 이해 04 - 분석 / 현행 시스템 분석</a><li><a href="/posts/02-process_modeling/">[IT] 프로세스 모델링 이해 02 - 프로세스 모델링 개요, 절차</a><li><a href="/posts/01-process_modeling/">[IT] 프로세스 모델링 이해 01 - 개요</a><li><a href="/posts/fmtc_02/">[FMTC] FMTC 원격주행 시스템 개발 02 - </a></ul></div><div id="access-tags"><div class="panel-heading">인기 태그</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/ubuntu/">ubuntu</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/jupyter-notebook/">Jupyter Notebook</a> <a class="post-tag" href="/tags/robot/">Robot</a> <a class="post-tag" href="/tags/ros/">ROS</a> <a class="post-tag" href="/tags/deeplearning/">DeepLearning</a> <a class="post-tag" href="/tags/machinelearning/">MachineLearning</a> <a class="post-tag" href="/tags/mechanical-engineering/">Mechanical Engineering</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">바로가기</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4 mt-5"><div id="related-posts" class="mb-2 mb-sm-4"><h3 class="pt-2 mb-4 ml-1" data-toc-skip>관련된 글</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/01-rl/"><div class="card-body"> <em class="small" data-ts="1629298800" data-df="YYYY-MM-DD" > 2021-08-19 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[RL] 강화 학습 개념과 마르코프 결정, 벨만 방정식, 정책 반복법, 모델 기반 강화학습</h3><div class="text-muted small"><p> 본 포스팅은 “Do it! 강화 학습 입문” - (이지스퍼블리싱, 조규남, 맹윤호, 임지순 지음)을 바탕으로 공부한 내용을 정리한 것입니다. 강화학습 마르코프 결정 (MDP, Markov Decision Process) 영화를 예시로 바라보는 마르코프 결정 상태, 행동, 보상 행동에 의한 상태 전이에...</p></div></div></a></div><div class="card"> <a href="/posts/01-dl/"><div class="card-body"> <em class="small" data-ts="1628521200" data-df="YYYY-MM-DD" > 2021-08-10 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[DL] Keras 설치</h3><div class="text-muted small"><p> 본 포스팅은 “케라스 창시자에게 배우는 딥러닝” - (길벗출판사, 프랑소와 숄레 지음, 박해선 옮김)을 바탕으로 공부한 내용을 정리한 것입니다. 딥러닝에서 ‘딥’은? 딥러닝의 특징 텐서플로우, 케라스 설치 가상환경 생성 Tensorflow, Keras 등 설치 1. 딥러닝에서 ‘딥’은?...</p></div></div></a></div><div class="card"> <a href="/posts/01-ml/"><div class="card-body"> <em class="small" data-ts="1629126000" data-df="YYYY-MM-DD" > 2021-08-17 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[ML] 파이썬 머신러닝 환경 구성</h3><div class="text-muted small"><p> 본 포스팅은 “파이썬 머신러닝 완벽가이드” - (위키북스, 권철민 지음)을 바탕으로 공부한 내용을 정리한 것입니다. 머신러닝, Python vs R Anaconda, Jupyter Notebook Anaconda 설치 설치 확인 Microsoft Visual Studio Build Tools ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/01-prj1/" class="btn btn-outline-primary" prompt="이전 글"><p>[KFQ] KFQ 1차 프로젝트 / 국내 영화관람객수 예측</p></a> <a href="/posts/01-prj2/" class="btn btn-outline-primary" prompt="다음 글"><p>[KFQ] KFQ 2차 프로젝트 / 수강생 관리 웹 사이트</p></a></div></div></div></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">인기 태그</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/ubuntu/">ubuntu</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/jupyter-notebook/">Jupyter Notebook</a> <a class="post-tag" href="/tags/robot/">Robot</a> <a class="post-tag" href="/tags/ros/">ROS</a> <a class="post-tag" href="/tags/deeplearning/">DeepLearning</a> <a class="post-tag" href="/tags/machinelearning/">MachineLearning</a> <a class="post-tag" href="/tags/mechanical-engineering/">Mechanical Engineering</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><footer><div class="container pl-lg-4 pr-lg-4"><div class="d-flex justify-content-between align-items-center text-muted ml-md-3 mr-md-3"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/19tak">19tak</a>. <span data-toggle="tooltip" data-placement="top" title="명시되지 않는 한 이 사이트의 블로그 게시물은 작성자의 Creative Commons Attribution 4.0 International(CC BY 4.0) 라이선스에 따라 사용이 허가되었습니다.">일부 권리 보유</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></div></footer><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">새 버전의 콘텐츠를 사용할 수 있습니다.</p><button type="button" class="btn btn-primary" aria-label="Update"> 업데이트 </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">검색 결과가 없습니다.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/ko.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
