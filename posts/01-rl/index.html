<!DOCTYPE html><html lang="ko-KR" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="[RL] 강화 학습 개념과 마르코프 결정, 벨만 방정식, 정책 반복법, 모델 기반 강화학습" /><meta property="og:locale" content="ko_KR" /><meta name="description" content="본 포스팅은 “Do it! 강화 학습 입문” - (이지스퍼블리싱, 조규남, 맹윤호, 임지순 지음)을 바탕으로 공부한 내용을 정리한 것입니다." /><meta property="og:description" content="본 포스팅은 “Do it! 강화 학습 입문” - (이지스퍼블리싱, 조규남, 맹윤호, 임지순 지음)을 바탕으로 공부한 내용을 정리한 것입니다." /><link rel="canonical" href="https://19tak.github.io/posts/01-rl/" /><meta property="og:url" content="https://19tak.github.io/posts/01-rl/" /><meta property="og:site_name" content="19tak" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-08-19T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[RL] 강화 학습 개념과 마르코프 결정, 벨만 방정식, 정책 반복법, 모델 기반 강화학습" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-10-16T16:26:29+09:00","datePublished":"2021-08-19T00:00:00+09:00","description":"본 포스팅은 “Do it! 강화 학습 입문” - (이지스퍼블리싱, 조규남, 맹윤호, 임지순 지음)을 바탕으로 공부한 내용을 정리한 것입니다.","headline":"[RL] 강화 학습 개념과 마르코프 결정, 벨만 방정식, 정책 반복법, 모델 기반 강화학습","mainEntityOfPage":{"@type":"WebPage","@id":"https://19tak.github.io/posts/01-rl/"},"url":"https://19tak.github.io/posts/01-rl/"}</script><title>[RL] 강화 학습 개념과 마르코프 결정, 벨만 방정식, 정책 반복법, 모델 기반 강화학습 | 19tak</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="19tak"><meta name="application-name" content="19tak"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="https://user-images.githubusercontent.com/84369912/127220954-701d3e97-6d9d-447a-b99f-1a31e4b7b03d.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title"> <a href="/">19tak</a></div><div class="site-subtitle">Peace n Love</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>홈</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>카테고리</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>태그</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>아카이브</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>정보</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/19tak" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://www.instagram.com/taaaktaaaak" aria-label="instagram" target="_blank" rel="noopener"> <i class="fab fa-instagram"></i> </a> <a href="https://www.youtube.com/channel/UCse2sGik0zU4fVNZyDjhcOQ" aria-label="youtube" target="_blank" rel="noopener"> <i class="fab fa-youtube"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['wjdxkrdl123','naver.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> 홈 </a> </span> <span>[RL] 강화 학습 개념과 마르코프 결정, 벨만 방정식, 정책 반복법, 모델 기반 강화학습</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> 포스트</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="검색..."> </span> <span id="search-cancel" >취소</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>[RL] 강화 학습 개념과 마르코프 결정, 벨만 방정식, 정책 반복법, 모델 기반 강화학습</h1><div class="post-meta text-muted"> <span> 게시 <em class="" data-ts="1629298800" data-df="YYYY-MM-DD" data-toggle="tooltip" data-placement="bottom"> 2021-08-19 </em> </span> <span> 업데이트 <em class="" data-ts="1665905189" data-df="YYYY-MM-DD" data-toggle="tooltip" data-placement="bottom"> 2022-10-16 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/19tak">19tak</a> </em> </span><div></div></div></div><div class="post-content"><p>본 포스팅은 <strong>“Do it! 강화 학습 입문”</strong> - (이지스퍼블리싱, 조규남, 맹윤호, 임지순 지음)을 바탕으로 공부한 내용을 정리한 것입니다.</p><ol><li>강화학습<li>마르코프 결정 (MDP, Markov Decision Process)<ul><li>영화를 예시로 바라보는 마르코프 결정<li>상태, 행동, 보상<li>행동에 의한 상태 전이에 확률 도입</ul><li>벨만 방정식 (Bellman Equation)<ul><li>보상 총합이 무한으로 발산하는 문제<li>시간에 따른 할인 개념 도입하기<li>보상 총합, 가치 함수<li>현재 상태의 가치 구하기, 벨만 방정식</ul><li>정책<ul><li>정책 반복법</ul></ol><hr /><h1 id="1-강화학습-reinforcement-learning">1. 강화학습 (Reinforcement Learning)</h1><p>머신러닝 (Machine Learning)은 크게 다음과 같이 나눌 수 있다.</p><ol><li>지도학습 (Supervised Learning)<ul><li>데이터와 레이블이 주어진 상태에서, 새 데이터에 레이블을 매기는 방법을 학습.</ul><li>비지도학습 (Un-Supervised Learning)<ul><li>레이블 없이 데이터만 주어진 상태에서, 새 데이터의 분류, 밀도 추정 등의 방법을 학습.<li>특징을 요약할 때 등.</ul><li>강화학습 (Reinforcement Learning)<ul><li>행동과 보상으로, 어떤 상태에서 최적의 행동을 찾아가는 방법을 학습.</ul></ol><p>1906년 <strong>마르코프 결정 과정(MDP, Markov Decision Process)</strong>, 1950년 <strong>동적 계획법 (DP, Dynamic Programming)</strong>의 오랜 역사 이후, 강화학습은 딥러닝과의 결합 (대표적으로 DQN, Deep Q-Network)으로 다시 주목 받고 있다.</p><h3 id="강화학습의-공부-방향"><span class="mr-2">강화학습의 공부 방향</span><a href="#강화학습의-공부-방향" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>강화학습의 기본인 <strong>MDP</strong>의 용어들과 그 의미, <strong>모델 기반 강화 학습</strong>, <strong>모델 프리 강화 학습</strong>, <strong>시간차 학습</strong>, <strong>Q 학습</strong>.</p><p>이후 <strong>CNN (Convolution Neural Network)</strong>과 결합한 <strong>DQN</strong>나 <strong>PPO</strong>, <strong>GPT</strong> 등으로 공부 방향을 잡으면 될 것 같다.</p><p>내가 내린 강화 학습의 짧막한 요약은 다음과 같다.</p><h3 id="강화-학습은-상태에서-행동에-대한-보상을-최적화-하는-함수-정책-policy을-찾는-것"><span class="mr-2">“강화 학습은 <strong>상태</strong>에서 <strong>행동</strong>에 대한 <strong>보상</strong>을 최적화 하는 함수, <strong>정책</strong> (Policy)을 찾는 것.”</span><a href="#강화-학습은-상태에서-행동에-대한-보상을-최적화-하는-함수-정책-policy을-찾는-것" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><blockquote><p>강화 학습은 지금 상태에서 해야 할 최적의 행동이 무엇인지 방향을 세워 나가는 과정이다.</p></blockquote><hr /><h1 id="2-마르코프-결정-mdp-markov-decision-process">2. 마르코프 결정 (MDP, Markov Decision Process)</h1><blockquote><p><strong>마르코프 결정 과정 (MDP, Markov Decision Process)</strong>은 의사 결정을 하는데 필요한 체계로 볼 수 있다.</p></blockquote><h2 id="영화를-예시로-바라보는-마르코프-결정"><span class="mr-2">영화를 예시로 바라보는 마르코프 결정</span><a href="#영화를-예시로-바라보는-마르코프-결정" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>책에서는 이를 설명하기 위해 예시로 영화 <strong><a href="https://movie.naver.com/movie/bi/mi/basic.naver?code=90589" title="네이버 영화 검색">엣지 오브 투모로우</a></strong>를 들고 있다.</p><blockquote><p><strong>엣지 오브 투모로우 (Edge of Tomorrow, 2014)</strong></p><p><img data-src="https://user-images.githubusercontent.com/84369912/130061856-d1bda510-dbed-42e5-bd5d-797845888a7e.png" alt="image" data-proofer-ignore> 주인공 빌 케이지(톰 크루즈)는 외계인이 점령한 지구를 해방하기 위한 인류 연합군에서 상륙작전에 투입되고, 우연히 외계인의 <strong>시간 리셋</strong> 능력을 얻고 죽을 때마다 작전의 전날에 부활한다. 케이지는 전투와 사망을 수백 번 반복하며, 인류의 승리를 위해 싸워나간다.</p></blockquote><p>강화학습의 관점에서 해당 줄거리를 바라보았을 때, 다음 그림과 같이 나타낼 수 있다.</p><p><img data-src="https://user-images.githubusercontent.com/84369912/130061782-5f5b3149-4bb3-40f8-9afc-0813863ade2f.png" alt="image" data-proofer-ignore></p><p>강화학습에서 <strong>에이전트 (Agent)</strong>는 해당 예시에서 행동의 주체인 주인공으로 볼 수 있다.</p><p><strong>에이전트</strong>는 전투라는 <strong>환경</strong>에서 <strong>상호 작용</strong>하며 가장 큰 <strong>보상</strong>인 외계인 대장 제거를 위해,</p><p>죽을 때마다 작전 전날에 부활하며 <strong>초기 상태</strong>에서 출발하여,</p><p>새로운 <strong>에피소드</strong>에서 다양한 <strong>상태</strong>와 그 때의 <strong>행동</strong>을 통해 대장을 물리치는 과정을 <strong>탐색</strong>한다.</p><p><img data-src="https://user-images.githubusercontent.com/84369912/130063613-786fcf98-ebab-41f7-a2c7-f4e52a5c374a.png" alt="image" data-proofer-ignore></p><p>\(s_{1}\)이나 \(a_{1-1}\)과 같은 표기는 각각 <strong>상태 (state)</strong>와 <strong>행동 (action</strong>)을 의미한다.</p><ul><li>본진(\(s_{1}\))에서 깨어난 에이전트는 상륙 작전 전투에 참가(\(a_{1-1}\)), 본진 이탈(\(a_{1-2}\))가 가능.<li>상륙 작전 투입(\(s_{2}\))된 에이전트는 혼자 전투(\(a_{2-1}\)), 전투를 도울 영웅을 구출(\(a_{2-2}\))가 가능.<li>본진 이탈(\(s_{5}\))한 에이전트는 다시 상륙 작전(\(a_{5-1}\))을 향할 수도, 비밀을 조사하러 후방 연구소로 이동(\(a_{5-2}\)) 가능.<li>후방 연구소 이동(\(s_{6}\))한 에이전트는 전투 훈련(\(a_{6-1}\))을 받을 수도, 외계인 보스 조사(\(a_{6-2}\))도 가능.</ul><h2 id="상태-행동-보상"><span class="mr-2">상태, 행동, 보상</span><a href="#상태-행동-보상" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>이런 식으로 에이전트는 시작 상태와 종단 상태를 포함한 7가지 상태,</p><p>그 둘을 제외한 각 상태에서 2개의 행동을 취할 수 있다.</p><p>각 상태 \(s_{1}, s_{2}, \ldots, s_{7}\)의 상태 집합을 \(S\)로 표현하고, 행동의 집합을 \(A(s)\)로 표현.</p><p>\(s_{1}\)에서 취할 수 있는 행동의 집합은 \(A(s_{1})\)이 된다.</p><p>그림에서 각 상태에서의 보상이 명시 되어있다.</p>\[R(s_{3})=-1, R(s_{4})=-1, R(s_{7})=1\] \[R(s_{1})=R(s_{2})=R(s_{5})=R(s_{6})=0\]<h2 id="행동에-의한-상태-전이에-확률-도입"><span class="mr-2">행동에 의한 상태 전이에 확률 도입</span><a href="#행동에-의한-상태-전이에-확률-도입" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>에이전트가 \(s_{1}\)에서 \(a_{1-2}\)를 했을 때, 상태 전이에는 확률이 있다.</p><p>80% 확률로 \(s_{5}\), 20% 확률로 \(s_{1}\)로 전이될 수도 있다.</p><p><strong>상태 전이 확률 (State Transition Probablity)</strong>는 수식으로 표현가능 하며, 상태-행동 한 쌍의 전이 확률 합은 1이다.</p>\[P(s_{5} \mid s_{1},a_{1-2})=0.8\] \[P(s_{1} \mid s_{1},a_{1-2})=0.2\]<p>이런 확률의 집합을 <strong>모델 (Model)</strong>이라 한다. 표로 이들을 표현하면 다음과 같다.</p><div class="table-wrapper"><table><tbody><tr><td>상태 (Status, S)<td>에이전트가 환경 내 특정 시점에서 관찰할 수 있는 것을 수치화<tr><td>행동 (Action, A)<td>에이전트가 환경에게 전달하는 입력<tr><td>보상 (Reward, R)<td>에이전트가 환경으로부터 전달받은 목적을 달성하기 위해 행동 수행에 대한 피드백<tr><td>모델 (Model, M)<td>행동에 따른 상태 전이가 일어날 확률을 담은 규칙</table></div><p>즉, <strong>MDP</strong>는 상태 집합 \(S\)로 이루어지며, 행동 집합 \(A\)가 있고,</p><p>상태 \(s\)에서 행동 \(a\)를 통해 상태 \(s^\prime\)로 전이할 확률은 \(P(s^\prime \mid s,a)\)이며, 보상은 \(R(s,a)\)이다.</p><blockquote><p>마르코프 특성 (Markov Property)를 만족한다고 가정하면 모든 상태는 직전의 상태와 행동에 의해서만 결정된다. MDP에서는 에이전트의 오직 현재 상태만 다음 상태 전이 확률에 영향을 미친다. ‘과거는 상관없다’</p></blockquote><hr /><h1 id="3-벨만-방정식-bellman-equation">3. 벨만 방정식 (Bellman Equation)</h1><p>장기적 보상의 예측치를 극대화하는 행동을 찾는 것. MDP 상태 안에서 최적의 행동을 찾는 방법.</p><p>에이전트가 특정 상태에서 미래 보상의 총합을 구할 수 있다고 가정하면,</p><p>보상의 예측치를 최대로 만드는 상태로 가는 행동이 곧 최적의 행동이다. 같이 식을 세워나가자.</p><h2 id="보상-총합이-무한으로-발산하는-문제"><span class="mr-2">보상 총합이 무한으로 발산하는 문제</span><a href="#보상-총합이-무한으로-발산하는-문제" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>예시로 한 에피소드에서 에이전트가 취할 수 있는 일부 상태 경로와 그에 따른 보상의 총합은 다음과 같다.</p><ul><li>상태 경로: \(s_{1} \rightarrow s_{2} \rightarrow s_{3}\)<li>보상 총합: \(R(s_{1})+R(s_{2})+R(s_{3})\)<li>상태 경로: \(s_{1} \rightarrow s_{5} \rightarrow s_{2} \rightarrow s_{3}\)<li>보상 총합: \(R(s_{1})+R(s_{5})+R(s_{2})+R(s_{3})\)<li>상태 경로: \(s_{1} \rightarrow s_{5} \rightarrow s_{1} \rightarrow s_{5} \ldots\)<li>보상 총합: \(R(s_{1})+R(s_{5})+R(s_{1})+R(s_{5})+\ldots\)</ul><p>이 때, 3번째 에피소드에서는 \(s_{1}\)과 \(s_{5}\)를 무한히 왕복하여, 보상 총합이 무한으로 발산하는 문제가 발생한다.</p><h2 id="시간에-따른-할인-개념-도입하기"><span class="mr-2">시간에 따른 할인 개념 도입하기</span><a href="#시간에-따른-할인-개념-도입하기" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>시간에 따른 <strong>할인 (Discount)</strong> 개념을 도입하며, 위에서 서술한 문제를 해결할 수 있다.</p><p>에이전트가 최대한 가까운 시점에 보상을 얻을 수 있도록 하는 것이다.</p><p>0과 1사이의 할인율을 감마 \(\gamma\)로 나타내면 다음과 같다.</p><ul><li>상태 경로: \(s_{1} \rightarrow s_{2} \rightarrow s_{3}\)<li>보상 총합: \(R(s_{1})+\gamma R(s_{2})+\gamma ^2 R(s_{3})\)<li>상태 경로: \(s_{1} \rightarrow s_{5} \rightarrow s_{2} \rightarrow s_{3}\)<li>보상 총합: \(R(s_{1})+\gamma R(s_{5})+\gamma ^2 R(s_{2})+\gamma ^3 R(s_{3})\)<li>상태 경로: \(s_{1} \rightarrow s_{5} \rightarrow s_{1} \rightarrow s_{5} \ldots\)<li>보상 총합: \(R(s_{1})+\gamma R(s_{5})+\gamma ^2 R(s_{1})+\gamma ^3 R(s_{5})+\ldots\)</ul><p>보상 \(R(s)\)의 값이 모두 1이라 가정하면, 보상 총합은 공비가 \(\gamma\)인 등비급수로 \(\mid \gamma \mid &lt; 1\)이기에 수렴한다.</p><p>따라서 보상 총합이 무한으로 발산하는 문제를 해결 할 수 있다.</p><blockquote><p>MDP에서 보상은 상태와 행동의 함수 \(R(s,a)\)와 마르코프 보상 과정 (MRP, Markov Reward Process)에서 사용하는 상태만의 함수인 \(R(s)\)는 다른 의미이지만, 할인율의 개념을 간단하게 보여주기 위해 사용했다.</p></blockquote><h2 id="보상-총합-가치-함수"><span class="mr-2">보상 총합, 가치 함수</span><a href="#보상-총합-가치-함수" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>우리는 주어진 상태에서 최적의 행동을 찾는 방법이 필요하다.</p><p>한 에피소드에서 얻을 수 있는 보상 총합이 아닌, 주어진 상태에서 미래에 얻을 수 있는 보상 총합을 구해야한다.</p><p><strong>이익 (Return)</strong>은 할인율과 t번째 상태를 이용해 수식으로 다음과 같이 나타낼 수 있다.</p>\[G_{t}=\sum \limits_{k=0}^\infty \gamma ^k R_{t+k+1}\]<p>이 때, 특정 상태에서 미래에 얻을 수 있는 보상의 총합, <strong>가치함수 (Value Function)</strong>는</p><p>특정 상태에서 갈 수 있는 전체 경로에 대한 평균, 이익의 기댓값이다.</p>\[V(s)=E[G_{t} \mid S_{t}=s]\]<div class="table-wrapper"><table><tbody><tr><td>보상 (R)<td>특정 상태에서 얻을 수 있는 즉각적 피드백<tr><td>이익 (G)<td>한 에피소드의 특정 상태에서 종단 상태까지 받을 수 있는 보상 총합<tr><td>가치 함수 (V)<td>특정 상태로부터 기대할 수 있는 보상</table></div><blockquote><p>예시로, 예방 주사를 맞을 때, 병원비와 아픔으로 즉각적인 보상은 음수. 장기적으로 건강이라는 가치를 제공하여 가치는 양수.</p></blockquote><h2 id="현재-상태의-가치-구하기-벨만-방정식"><span class="mr-2">현재 상태의 가치 구하기, 벨만 방정식</span><a href="#현재-상태의-가치-구하기-벨만-방정식" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>현재 상태의 가치의 기댓값을 구하는 과정은 다음과 같다.</p><ol><li>종단 상태를 제외한 모든 상태의 가치를 0으로 초기화한다.<ul><li>종단 상태는 상태 전이가 없으니, <code class="language-plaintext highlighter-rouge">보상=가치</code>이다.</ul><li>종단 상태와 인접한 상태의 가치를 구한다.<li>행동의 기대 가치에 할인율을 곱한 다음 더한다.</ol><p>앞서 보여준 영화 예시 그림으로 풀어 나가면 다음과 같다.</p><div class="table-wrapper"><table><tbody><tr><td>상태 \(s_{2}\)에서 <br /> 행동 \(a_{2-1}\) 선택<td>\(s_{3}\)으로 전이할 확률: \(P(s_{3} \mid s_{2},a_{2-1})=1\) <br /> \(s_{3}\)의 가치: \(-0\) <br /> 가치의 기댓값: \(s_{3}\)의 보상 + \(s_{3}\)의 가치 \(0\) \(\times\) 전이 확률 \(1\) \(\times\) 할인율\(0.9=-1\)<tr><td>상태 \(s_{2}\)에서 <br /> 행동 \(a_{2-2}\) 선택<td>\(s_{4}\)으로 전이할 확률: \(P(s_{4} \mid s_{2},a_{2-2})=0.6\) <br /> \(s_{7}\)으로 전이할 확률: \(P(s_{7} \mid s_{2},a_{2-2})=0.4\) <br /> \(s_{4}\)의 가치: \(0\) <br /> \(s_{7}\)의 가치: \(0\) <br /> 가치의 기댓값: [전이 확률 \(0.6\) \(\times\) (\(s_{4}\)의 보상 \(-1\) + \(s_{4}\)의 가치 \(0\) \(\times\) 할인율 \(0.9\)) <br /> + 전이 확률 0.4 \(\times\) (\(s_{7}\)의 보상 \(1\) + \(s_{7}\)의 가치 \(0\) \(\times\) 할인율 \(0.9\))]\(=-0.2\)</table></div><p>이 과정을 수식으로 표현하면 다음과 같은 <strong>벨만 방정식 (Bellman Equation)</strong>이 된다.</p>\[V(s)=max_{a} \sum \limits_{s^\prime} P(s^\prime \mid s,a)[R(s,a)+\gamma V(s^\prime)]\]<p>벨만 방정식은 다음 상태의 가치를 사용해서 현재 상태의 가치를 구한다.</p><p>현재를 구하기 위해 다음 상태를 계속 추적하여, 결국 종단 상태에 도달한다.</p><p>이를 코드로 구현하면 <strong>동적 계획법 (Dynamic Programming)</strong>이라는 알고리즘을 사용한다.</p><p>이처럼 동적 계획법을 기반으로 가치 함수를 산출하는 방식을 <strong>가치 반복법 (Value Iteration)</strong>이라 한다.</p><p>이제 우리는 미래 보상을 최대화하는 최적 행동을 선택 할 수 있다.</p><p>이렇게 상태를 입력으로 받아 최적 행동을 출력하는 함수를 <strong>정책 (Policy)</strong>이라 한다..</p><h3 id="이-정책이-강화-학습에서-구하고자-하는-목표이다"><span class="mr-2">이 <strong>정책</strong>이 강화 학습에서 구하고자 하는 목표이다.</span><a href="#이-정책이-강화-학습에서-구하고자-하는-목표이다" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><hr /><h1 id="4-정책">4. 정책</h1><p><strong>정책</strong>이란 지금 상태에서 해야 할 최적의 행동이 무엇인지 알려주는 것이다.</p>\[\pi (s)=a\]<p>강화 학습은 이 정책을 세워 나가는 과정이다.</p><h2 id="가치-반복법"><span class="mr-2">가치 반복법</span><a href="#가치-반복법" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><strong>가치 반복법</strong>을 통해 수렴한 가치 함수는 <strong>최적의 가치 함수</strong> (Optimal Value Function)는 \(V\pi (s)\) 이다.</p><p>이를 이용해 벨만 방정식을 조금 수정하면 다음과 같다.</p>\[\pi (s)=\gamma \sum \limits_{s^\prime} p(s,a) V_{\pi}(s^\prime)\]<blockquote><p>가치 함수가 수렴 할 때까지 반복하기 보다는, 가치 갱신값 차이가 일정한 임곗값 이내에 들어오면 반복을 종료하는 방식을 사용하기도 한다.</p></blockquote><p>최적의 정책 함수는 가치 함수를 업데이트 하는 가치 반복법 외에도, 정책 함수 자체를 업데이트하는 방식으로도 구할 수 있다.</p><h2 id="정책-반복법"><span class="mr-2">정책 반복법</span><a href="#정책-반복법" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="정책-평가-policy-evaluation"><span class="mr-2">정책 평가 (Policy Evaluation)</span><a href="#정책-평가-policy-evaluation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>정책 \(\pi (s)\)는 처음에 무작위로 초기화한다.</p><p>\(\pi (s)\)가 주어진 것은 에이전트가 어떤 상황에서 어떤 행동을 해야할지 지시받았음을 의미한다.</p><p>상태 \(s\)에서 선택할 수 있는 모든 행동에 대해 이익의 기댓값을 계산하고 최대치를 고르는 것이 아니라,</p><p>주어진 행동 \(\pi (s)\)에 대한 이익의 기댓값만 계산하면 된다.</p><p>주어진 정책에 따라 가치 함수를 구하는 일을 <strong>정책 평가 (Policy Evaluation)</strong>라 한다.</p><h3 id="정책-개선-policy-improvement"><span class="mr-2">정책 개선 (Policy Improvement)</span><a href="#정책-개선-policy-improvement" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>가치 함수 계산을 통해, 각 상태에서 정책 \(\pi (s)\)를 따라 행동했을 때와</p><p>정책과 아무 관련 없는 다른 행동을 선택했을 때 어느 쪽 가치가 더 큰지 알 수 있다.</p><p>정책과 무관한 다른 행동의 가치가 클 경우,</p><p>기존 정책을 가치가 더 큰 새로운 행동으로 업데이트하는 것을 <strong>정책 개선 (Policy Improvement)</strong>이라 한다.</p><blockquote><p>정책 평가는 정책 함수를 기반으로 가치 함수를 갱신하고,</p><p>정책 개선은 가치 함수를 기반으로 정책 함수를 갱신한다.</p></blockquote><p>정책 평가와 정책 개선을 반복하여, 가치 함수와 정책 함수가 변화하지 않는,</p><p>가치와 정책이 모두 수렴하게 하는 이것을 <strong>정책 반복법</strong>이라 한다.</p><p>\(E \rightarrow\)는 정책 평가, \(I \rightarrow\)를 정책 개선으로 하여 수식으로 표현하면 다음과 같다.</p>\[\pi _{0} E \rightarrow v_{\pi 0} I \rightarrow \pi _{1} E \rightarrow v_{\pi 1} I \rightarrow \ldots I \rightarrow \pi _{*} E \rightarrow v_{*}\]<p>정책 반복법은 가치 반복법 알고리즘보다 반복문도 깊고, 알고리즘 자체도 복잡하지만,</p><p>상황에 따라 더 빠르게 최적의 정책 함수로 수렴하기도하여, 둘 중 절대적 우위는 없다.</p><hr /><p>또한, 이들은 에이전트가 환경 안에서 가질 수 있는</p><p>모든 상태와 각 상태 간의 전이 확률을 미리 안다는 전제가 깔려 있다.</p><p>MDP를 이루는 구성 요소 중 모델에 대한 모든 지식을 미리 알고 있다는 것이다.</p><p>종단 상태로부터 역추적해 모든 상태와 행동의 가치를 계산하는 완전 탐색이라는 특징까지도,</p><p>가치 반복법과 정책 반복법은 현실 세계에 적용하기 어렵다.</p><hr /><h1 id="참고">참고</h1><p><strong>“Do it! 강화 학습 입문”</strong> - (이지스퍼블리싱, 조규남, 맹윤호, 임지순 지음)</p><ul><li>깃허브: <a href="https://github.com/yunho0130/start-RL">https://github.com/yunho0130/start-RL</a></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/study/'>Study</a>, <a href='/categories/ai/'>AI</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a> <a href="/tags/reinforcementlearning/" class="post-tag no-text-decoration" >ReinforcementLearning</a> <a href="/tags/mdp/" class="post-tag no-text-decoration" >MDP</a> <a href="/tags/markov-decision-process/" class="post-tag no-text-decoration" >Markov Decision Process</a> <a href="/tags/markov/" class="post-tag no-text-decoration" >Markov</a> <a href="/tags/bellman/" class="post-tag no-text-decoration" >Bellman</a> <a href="/tags/bellman-equation/" class="post-tag no-text-decoration" >Bellman Equation</a> <a href="/tags/model-based-reinforcement-learning/" class="post-tag no-text-decoration" >Model Based Reinforcement Learning</a> <a href="/tags/model-based/" class="post-tag no-text-decoration" >Model Based</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> 이 기사는 저작권자의 <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> 라이센스를 따릅니다.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">공유하기</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%5BRL%5D+%EA%B0%95%ED%99%94+%ED%95%99%EC%8A%B5+%EA%B0%9C%EB%85%90%EA%B3%BC+%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84+%EA%B2%B0%EC%A0%95%2C+%EB%B2%A8%EB%A7%8C+%EB%B0%A9%EC%A0%95%EC%8B%9D%2C+%EC%A0%95%EC%B1%85+%EB%B0%98%EB%B3%B5%EB%B2%95%2C+%EB%AA%A8%EB%8D%B8+%EA%B8%B0%EB%B0%98+%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5+-+19tak&url=https%3A%2F%2F19tak.github.io%2Fposts%2F01-rl%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%5BRL%5D+%EA%B0%95%ED%99%94+%ED%95%99%EC%8A%B5+%EA%B0%9C%EB%85%90%EA%B3%BC+%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84+%EA%B2%B0%EC%A0%95%2C+%EB%B2%A8%EB%A7%8C+%EB%B0%A9%EC%A0%95%EC%8B%9D%2C+%EC%A0%95%EC%B1%85+%EB%B0%98%EB%B3%B5%EB%B2%95%2C+%EB%AA%A8%EB%8D%B8+%EA%B8%B0%EB%B0%98+%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5+-+19tak&u=https%3A%2F%2F19tak.github.io%2Fposts%2F01-rl%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2F19tak.github.io%2Fposts%2F01-rl%2F&text=%5BRL%5D+%EA%B0%95%ED%99%94+%ED%95%99%EC%8A%B5+%EA%B0%9C%EB%85%90%EA%B3%BC+%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84+%EA%B2%B0%EC%A0%95%2C+%EB%B2%A8%EB%A7%8C+%EB%B0%A9%EC%A0%95%EC%8B%9D%2C+%EC%A0%95%EC%B1%85+%EB%B0%98%EB%B3%B5%EB%B2%95%2C+%EB%AA%A8%EB%8D%B8+%EA%B8%B0%EB%B0%98+%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5+-+19tak" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="링크 복사하기" data-title-succeed="링크가 복사되었습니다!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">최근 업데이트</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/03-process_modeling/">[IT] 프로세스 모델링 이해 03 - 분석 개요</a><li><a href="/posts/04-process_modeling/">[IT] 프로세스 모델링 이해 04 - 분석 / 현행 시스템 분석</a><li><a href="/posts/02-process_modeling/">[IT] 프로세스 모델링 이해 02 - 프로세스 모델링 개요, 절차</a><li><a href="/posts/01-process_modeling/">[IT] 프로세스 모델링 이해 01 - 개요</a><li><a href="/posts/fmtc_02/">[FMTC] FMTC 원격주행 시스템 개발 02 - </a></ul></div><div id="access-tags"><div class="panel-heading">인기 태그</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/ubuntu/">ubuntu</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/jupyter-notebook/">Jupyter Notebook</a> <a class="post-tag" href="/tags/robot/">Robot</a> <a class="post-tag" href="/tags/ros/">ROS</a> <a class="post-tag" href="/tags/deeplearning/">DeepLearning</a> <a class="post-tag" href="/tags/machinelearning/">MachineLearning</a> <a class="post-tag" href="/tags/mechanical-engineering/">Mechanical Engineering</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">바로가기</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4 mt-5"><div id="related-posts" class="mb-2 mb-sm-4"><h3 class="pt-2 mb-4 ml-1" data-toc-skip>관련된 글</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/02-rl/"><div class="card-body"> <em class="small" data-ts="1629385200" data-df="YYYY-MM-DD" > 2021-08-20 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[RL] 모델 프리 강화 학습</h3><div class="text-muted small"><p> 본 포스팅은 “Do it! 강화 학습 입문” - (이지스퍼블리싱, 조규남, 맹윤호, 임지순 지음)을 바탕으로 공부한 내용을 정리한 것입니다. 모델 기반 vs 모델 프리 몬테카를로 학습 (MC, Monte Carlo Learning) 몬테카를로 학습의 개념 몬테카를로 학습의 특징 시간차 학습 (...</p></div></div></a></div><div class="card"> <a href="/posts/01-dl/"><div class="card-body"> <em class="small" data-ts="1628521200" data-df="YYYY-MM-DD" > 2021-08-10 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[DL] Keras 설치</h3><div class="text-muted small"><p> 본 포스팅은 “케라스 창시자에게 배우는 딥러닝” - (길벗출판사, 프랑소와 숄레 지음, 박해선 옮김)을 바탕으로 공부한 내용을 정리한 것입니다. 딥러닝에서 ‘딥’은? 딥러닝의 특징 텐서플로우, 케라스 설치 가상환경 생성 Tensorflow, Keras 등 설치 1. 딥러닝에서 ‘딥’은?...</p></div></div></a></div><div class="card"> <a href="/posts/01-ml/"><div class="card-body"> <em class="small" data-ts="1629126000" data-df="YYYY-MM-DD" > 2021-08-17 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[ML] 파이썬 머신러닝 환경 구성</h3><div class="text-muted small"><p> 본 포스팅은 “파이썬 머신러닝 완벽가이드” - (위키북스, 권철민 지음)을 바탕으로 공부한 내용을 정리한 것입니다. 머신러닝, Python vs R Anaconda, Jupyter Notebook Anaconda 설치 설치 확인 Microsoft Visual Studio Build Tools ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/03-ml/" class="btn btn-outline-primary" prompt="이전 글"><p>[ML] 머신러닝의 기본 개념과 Numpy</p></a> <a href="/posts/01-prj1/" class="btn btn-outline-primary" prompt="다음 글"><p>[KFQ] KFQ 1차 프로젝트 / 국내 영화관람객수 예측</p></a></div></div></div></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">인기 태그</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/ubuntu/">ubuntu</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/jupyter-notebook/">Jupyter Notebook</a> <a class="post-tag" href="/tags/robot/">Robot</a> <a class="post-tag" href="/tags/ros/">ROS</a> <a class="post-tag" href="/tags/deeplearning/">DeepLearning</a> <a class="post-tag" href="/tags/machinelearning/">MachineLearning</a> <a class="post-tag" href="/tags/mechanical-engineering/">Mechanical Engineering</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><footer><div class="container pl-lg-4 pr-lg-4"><div class="d-flex justify-content-between align-items-center text-muted ml-md-3 mr-md-3"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/19tak">19tak</a>. <span data-toggle="tooltip" data-placement="top" title="명시되지 않는 한 이 사이트의 블로그 게시물은 작성자의 Creative Commons Attribution 4.0 International(CC BY 4.0) 라이선스에 따라 사용이 허가되었습니다.">일부 권리 보유</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></div></footer><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">새 버전의 콘텐츠를 사용할 수 있습니다.</p><button type="button" class="btn btn-primary" aria-label="Update"> 업데이트 </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">검색 결과가 없습니다.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/ko.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
